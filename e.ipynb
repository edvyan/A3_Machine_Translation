{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. The English-Swedish dataset used is available on GitHub in the repository 'NLP-Swedish-English' under the directory 'dat410_europarl'. This repository is a user-contributed project by the GitHub user 'sivajeet'. The link of the repository is as follows: \n",
        "https://github.com/sivajeet/NLP-Swedish-English/tree/main/dat410_europarl "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Dataset preparation involves steps as described below: \n",
        "\n",
        "Tokenization:\n",
        "Tokenization is the process of splitting the text into smaller units, usually words or subwords. In the code, tokenization is performed using the spaCy library for English (en_core_web_sm) and Swedish (sv_core_news_sm). SpaCy is a used for tokenization along with other linguistic annotations. The get_tokenizer function from torchtext.data.utils is utilized to obtain tokenizers for both languages.\n",
        "\n",
        "For English, the tokenizer splits the text into individual words and punctuation marks, considering the linguistic rules of English.\n",
        "For Swedish, the tokenizer handles the language-specific rules for tokenization, which may include compound words and special characters unique to Swedish.\n",
        "Word Segmentation (for languages with specific requirements):\n",
        "\n",
        "After tokenization, the next step is to build the vocabulary for both the source (English) and target (Swedish) languages. This involves creating a mapping from tokens (words or subwords) to unique indices and vice versa. The build_vocab_from_iterator function from torchtext.vocab is used for this purpose, which constructs the vocabulary based on the tokens obtained from the training data. The vocabulary includes special tokens such as <unk> (unknown), <pad> (padding), <sos> (start of sequence), and <eos> (end of sequence) to handle out-of-vocabulary words and sequence boundary indicators.\n",
        "\n",
        "Data Loading and Batching:\n",
        "Once the vocabulary is built, the dataset is loaded into PyTorch DataLoader objects for efficient processing. The collate_batch function is used to collate data samples into batches, where each sample consists of a pair of source and target sentences. The sentences are tokenized, numericalized (converted to indices using the vocabulary), and padded to ensure uniform length within each batch. Finally, the data loaders are ready to be used for training, validation, and testing of the translation model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KVudoD9w2oMF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch, torchdata, torchtext\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import spacy\n",
        "import random, math, time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0ARSd4Wkb6K",
        "outputId": "272e102d-a5ea-4845-fc7d-94d541ff3f87"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 467, in _make_request\n",
            "    self._validate_conn(conn)\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 1099, in _validate_conn\n",
            "    conn.connect()\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py\", line 653, in connect\n",
            "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py\", line 806, in _ssl_wrap_socket_and_match_hostname\n",
            "    ssl_sock = ssl_wrap_socket(\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\ssl_.py\", line 465, in ssl_wrap_socket\n",
            "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\ssl_.py\", line 509, in _ssl_wrap_socket_impl\n",
            "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py\", line 517, in wrap_socket\n",
            "    return self.sslsocket_class._create(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py\", line 1108, in _create\n",
            "    self.do_handshake()\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py\", line 1383, in do_handshake\n",
            "    self._sslobj.do_handshake()\n",
            "ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 793, in urlopen\n",
            "    response = self._make_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 491, in _make_request\n",
            "    raise new_e\n",
            "urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py\", line 486, in send\n",
            "    resp = conn.urlopen(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 847, in urlopen\n",
            "    retries = retries.increment(\n",
            "              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 515, in increment\n",
            "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/compatibility.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)')))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\cli\\_util.py\", line 87, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\click\\core.py\", line 1157, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\typer\\core.py\", line 778, in main\n",
            "    return _main(\n",
            "           ^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\typer\\core.py\", line 216, in _main\n",
            "    rv = self.invoke(ctx)\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\click\\core.py\", line 1688, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\click\\core.py\", line 1434, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\click\\core.py\", line 783, in invoke\n",
            "    return __callback(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\typer\\main.py\", line 683, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\cli\\download.py\", line 43, in download_cli\n",
            "    download(model, direct, sdist, *ctx.args)\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\cli\\download.py\", line 77, in download\n",
            "    compatibility = get_compatibility()\n",
            "                    ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\cli\\download.py\", line 122, in get_compatibility\n",
            "    r = requests.get(about.__compatibility__)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py\", line 73, in get\n",
            "    return request(\"get\", url, params=params, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py\", line 59, in request\n",
            "    return session.request(method=method, url=url, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py\", line 517, in send\n",
            "    raise SSLError(e, request=request)\n",
            "requests.exceptions.SSLError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/compatibility.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)')))\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download sv_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhFjtLcR2oMH",
        "outputId": "d10c6836-b9d3-454c-81d6-e758561e4bcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "#make our work comparable if restarted the kernel\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KubuacwX2oMI"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "\n",
        "MAX_LENGTH = 100  # Define your maximum sentence length\n",
        "\n",
        "def read_and_truncate_data(file_path, max_length):\n",
        "    with io.open(file_path, encoding='utf-8') as f:\n",
        "        # Split the file into lines and truncate each line if necessary\n",
        "        return [line[:max_length].strip() for line in f]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PukRxRZf2oMI"
      },
      "outputs": [],
      "source": [
        "# Load and truncate the English and Swedish data\n",
        "english_data = read_and_truncate_data('en.txt', MAX_LENGTH)\n",
        "swedish_data = read_and_truncate_data('sv.txt', MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mr7NBh0q2oMJ"
      },
      "outputs": [],
      "source": [
        "# Pair the English and Swedish sentences\n",
        "paired_data = [(en, sv) for en, sv in zip(english_data, swedish_data) if len(en) <= MAX_LENGTH and len(sv) <= MAX_LENGTH]\n",
        "\n",
        "# Split data into training, validation, and test sets\n",
        "random.seed(SEED)\n",
        "random.shuffle(paired_data)\n",
        "total_size = len(paired_data)\n",
        "train_size = int(0.7 * total_size)\n",
        "val_size = int(0.2 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "train = paired_data[:train_size]\n",
        "val = paired_data[train_size:train_size+val_size]\n",
        "test = paired_data[train_size+val_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t_aKfZM2oMJ",
        "outputId": "4ab158fd-df9a-47ef-971b-9269bf7022f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6998"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSEtTDg_2oMJ",
        "outputId": "ad033ddd-7e3c-4272-aff9-2837bc6e8bc9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('through his management , commissioner monti achieves good results , as did his predecessor and , cle',\n",
              " 'kommissionär monti har uppnått goda resultat i sitt arbete , precis som sin föregångare , och har ty')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample = next(iter(train))\n",
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 467, in _make_request\n",
            "    self._validate_conn(conn)\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 1099, in _validate_conn\n",
            "    conn.connect()\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py\", line 653, in connect\n",
            "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py\", line 806, in _ssl_wrap_socket_and_match_hostname\n",
            "    ssl_sock = ssl_wrap_socket(\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\ssl_.py\", line 465, in ssl_wrap_socket\n",
            "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\ssl_.py\", line 509, in _ssl_wrap_socket_impl\n",
            "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py\", line 517, in wrap_socket\n",
            "    return self.sslsocket_class._create(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py\", line 1108, in _create\n",
            "    self.do_handshake()\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py\", line 1383, in do_handshake\n",
            "    self._sslobj.do_handshake()\n",
            "ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 793, in urlopen\n",
            "    response = self._make_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 491, in _make_request\n",
            "    raise new_e\n",
            "urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py\", line 486, in send\n",
            "    resp = conn.urlopen(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 847, in urlopen\n",
            "    retries = retries.increment(\n",
            "              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 515, in increment\n",
            "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/compatibility.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)')))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\cli\\_util.py\", line 87, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\click\\core.py\", line 1157, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\typer\\core.py\", line 778, in main\n",
            "    return _main(\n",
            "           ^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\typer\\core.py\", line 216, in _main\n",
            "    rv = self.invoke(ctx)\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\click\\core.py\", line 1688, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\click\\core.py\", line 1434, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\click\\core.py\", line 783, in invoke\n",
            "    return __callback(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\typer\\main.py\", line 683, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\cli\\download.py\", line 43, in download_cli\n",
            "    download(model, direct, sdist, *ctx.args)\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\cli\\download.py\", line 77, in download\n",
            "    compatibility = get_compatibility()\n",
            "                    ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\cli\\download.py\", line 122, in get_compatibility\n",
            "    r = requests.get(about.__compatibility__)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py\", line 73, in get\n",
            "    return request(\"get\", url, params=params, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py\", line 59, in request\n",
            "    return session.request(method=method, url=url, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py\", line 517, in send\n",
            "    raise SSLError(e, request=request)\n",
            "requests.exceptions.SSLError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/compatibility.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)')))\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-3Qq4lu2oMJ",
        "outputId": "af89d4c4-bdd5-43a1-a575-208c48d9ada1"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m SRC_LANGUAGE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m TRG_LANGUAGE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Swedish language code\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m token_transform[SRC_LANGUAGE] \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspacy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m token_transform[TRG_LANGUAGE] \u001b[38;5;241m=\u001b[39m get_tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacy\u001b[39m\u001b[38;5;124m'\u001b[39m, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msv_core_news_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Update for Swedish\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#example of tokenization of the english part\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\data\\utils.py:94\u001b[0m, in \u001b[0;36mget_tokenizer\u001b[1;34m(tokenizer, language)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m     spacy \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# Model shortcuts no longer work in spaCy 3.0+, try using fullnames\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# List is from https://github.com/explosion/spaCy/blob/b903de3fcb56df2f7247e5b6cfa6b66f4ff02b62/spacy/errors.py#L789\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     OLD_MODEL_SHORTCUTS \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     99\u001b[0m         spacy\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mOLD_MODEL_SHORTCUTS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(spacy\u001b[38;5;241m.\u001b[39merrors, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOLD_MODEL_SHORTCUTS\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    100\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\A443696\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
            "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ],
      "source": [
        "## 3. Preprocessing\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}\n",
        "# Update the tokenization for Swedish\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "SRC_LANGUAGE = 'en'\n",
        "TRG_LANGUAGE = 'sv'  # Swedish language code\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "token_transform[TRG_LANGUAGE] = get_tokenizer('spacy', language='sv_core_news_sm')  # Update for Swedish\n",
        "\n",
        "#example of tokenization of the english part\n",
        "print(\"Sentence: \", sample[0])\n",
        "print(\"Tokenization: \", token_transform[SRC_LANGUAGE](sample[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ax0CwOwQ2oMK"
      },
      "outputs": [],
      "source": [
        "# helper function to yield list of tokens\n",
        "# here data can be `train` or `val` or `test`\n",
        "def yield_tokens(data, language):\n",
        "    language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data:\n",
        "        yield token_transform[language](data_sample[language_index[language]]) #either first or second index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_oWJWN3o2oMK"
      },
      "outputs": [],
      "source": [
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
        "    # Create torchtext's Vocab object\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train, ln),\n",
        "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True) #indicates whether to insert symbols at the beginning or at the end\n",
        "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
        "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
        "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
        "    vocab_transform[ln].set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz6udYBj2oMK",
        "outputId": "a7031e3d-710a-4f94-dd7a-324fa10236d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[135, 9, 14, 0, 14]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#see some example\n",
        "vocab_transform[SRC_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LGH9PF5S2oML"
      },
      "outputs": [],
      "source": [
        "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-kwmjQPo2oML",
        "outputId": "a8318bb6-6632-459b-8418-f7fa81d32c50"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'kinds'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#print 1816, for example\n",
        "mapping[1891]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5KlCgnGL2oML",
        "outputId": "17d508b3-2517-41a4-ee6a-f87ecfdd0afe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<unk>'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#let's try unknown vocab\n",
        "mapping[0]\n",
        "#they will all map to <unk> which has 0 as integer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PfcMfTQ2oML",
        "outputId": "9af9c8b9-1336-4626-8630-564b21fc9e9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('<pad>', '<sos>', '<eos>')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#let's try special symbols\n",
        "mapping[1], mapping[2], mapping[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoEkkDVf2oMM",
        "outputId": "ecda356c-030d-49de-b9d5-7d4cc76502bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3131"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#check unique vocabularies\n",
        "len(mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "p_7ZW2_22oMM"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids):\n",
        "    return torch.cat((torch.tensor([SOS_IDX]), \n",
        "                      torch.tensor(token_ids), \n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# src and trg language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_batch(batch):\n",
        "    src_batch, src_len_batch, trg_batch = [], [], []\n",
        "    for example in batch:\n",
        "        src_sample = example['en']\n",
        "        trg_sample = example['th']\n",
        "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
        "        src_batch.append(processed_text)\n",
        "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
        "        src_len_batch.append(processed_text.size(0))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first = True)\n",
        "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first = True)\n",
        "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lLAhkIR2oMM",
        "outputId": "f61748be-6a38-4f79-da5b-cad478597967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English shape:  torch.Size([64, 15])\n",
            "Swedish shape:  torch.Size([64, 14])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# create DataLoader on train, test, and validation\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "valid_loader = DataLoader(val,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "# Sample Output\n",
        "for en, _, zh in train_loader:\n",
        "    break\n",
        "\n",
        "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
        "print(\"Swedish shape: \", zh.shape)  # (batch_size, seq len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Xv94o9_M2oMM"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
        "        super().__init__()\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
        "        self.dropout              = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        #src: [batch_size, src len, hid dim]\n",
        "\n",
        "        _src    = self.feedforward(src)\n",
        "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        #src: [batch_size, src len, hid dim]\n",
        "\n",
        "        return src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OOy6DJnP2oMM"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length = 100):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
        "                                           for _ in range(n_layers)])\n",
        "        self.dropout       = nn.Dropout(dropout)\n",
        "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "\n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        src_len    = src.shape[1]\n",
        "\n",
        "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        #pos: [batch_size, src_len]\n",
        "\n",
        "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        #src: [batch_size, src_len, hid_dim]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        #src: [batch_size, src_len, hid_dim]\n",
        "\n",
        "        return src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uQ2SiEu42oMN"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        assert hid_dim % n_heads == 0\n",
        "        self.hid_dim  = hid_dim\n",
        "        self.n_heads  = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "\n",
        "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "        self.dropout  = nn.Dropout(dropout)\n",
        "\n",
        "        self.scale    = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        #src, src, src, src_mask\n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        #Q=K=V: [batch_size, src len, hid_dim]\n",
        "\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        #Q = [batch_size, n heads, query len, head_dim]\n",
        "\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        #Q = [batch_size, n heads, query len, head_dim] @ K = [batch_size, n heads, head_dim, key len]\n",
        "        #energy = [batch_size, n heads, query len, key len]\n",
        "\n",
        "        #for making attention to padding to 0\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "        #attention = [batch_size, n heads, query len, key len]\n",
        "\n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        #[batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
        "        #x = [batch_size, n heads, query len, head dim]\n",
        "\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
        "        #x = [batch_size, query len, n heads, head dim]\n",
        "\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        #x = [batch_size, query len, hid dim]\n",
        "\n",
        "        x = self.fc_o(x)\n",
        "        #x = [batch_size, query len, hid dim]\n",
        "\n",
        "        return x, attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0wcyqJyQ2oMN"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = [batch size, src len, hid dim]\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-Lyz7TsL2oMN"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
        "        super().__init__()\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
        "        self.dropout              = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "\n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "        #trg = [batch_size, trg len, hid dim]\n",
        "\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "        #trg = [batch_size, trg len, hid dim]\n",
        "        #attention = [batch_size, n heads, trg len, src len]\n",
        "\n",
        "        _trg = self.feedforward(trg)\n",
        "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        #trg = [batch_size, trg len, hid dim]\n",
        "\n",
        "        return trg, attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "s6HIP9vs2oMN"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hid_dim, n_layers, n_heads,\n",
        "                 pf_dim, dropout, device,max_length = 100):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
        "                                            for _ in range(n_layers)])\n",
        "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout       = nn.Dropout(dropout)\n",
        "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "\n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len    = trg.shape[1]\n",
        "\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        #pos: [batch_size, trg len]\n",
        "\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "        #trg: [batch_size, trg len, hid dim]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        #trg: [batch_size, trg len, hid dim]\n",
        "        #attention: [batch_size, n heads, trg len, src len]\n",
        "\n",
        "        output = self.fc_out(trg)\n",
        "        #output = [batch_size, trg len, output_dim]\n",
        "\n",
        "        return output, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nT1Nm8P2oMO",
        "outputId": "af1fddf1-bdf1-40f6-883b-95649cd390d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "\n",
        "        #src = [batch size, src len]\n",
        "\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "\n",
        "        #trg = [batch size, trg len]\n",
        "\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "\n",
        "        trg_len = trg.shape[1]\n",
        "\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "\n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "\n",
        "        return output, attention\n",
        "\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Z_KqHgoE2oMO"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
        "OUTPUT_DIM = len(vocab_transform[TRG_LANGUAGE])\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "enc = Encoder(INPUT_DIM,\n",
        "              HID_DIM,\n",
        "              ENC_LAYERS,\n",
        "              ENC_HEADS,\n",
        "              ENC_PF_DIM,\n",
        "              ENC_DROPOUT,\n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM,\n",
        "              HID_DIM,\n",
        "              DEC_LAYERS,\n",
        "              DEC_HEADS,\n",
        "              DEC_PF_DIM,\n",
        "              DEC_DROPOUT,\n",
        "              device)\n",
        "\n",
        "SRC_PAD_IDX = PAD_IDX\n",
        "TRG_PAD_IDX = PAD_IDX\n",
        "\n",
        "\n",
        "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
        "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
        "hid_dim = 256\n",
        "enc_layers = 3\n",
        "dec_layers = 3\n",
        "enc_heads = 8\n",
        "dec_heads = 8\n",
        "enc_pf_dim = 512\n",
        "dec_pf_dim = 512\n",
        "enc_dropout = 0.1\n",
        "dec_dropout = 0.1\n",
        "\n",
        "SRC_PAD_IDX = PAD_IDX\n",
        "TRG_PAD_IDX = PAD_IDX\n",
        "\n",
        "enc = Encoder(input_dim,\n",
        "              hid_dim,\n",
        "              enc_layers,\n",
        "              enc_heads,\n",
        "              enc_pf_dim,\n",
        "              enc_dropout,\n",
        "              device)\n",
        "\n",
        "dec = Decoder(output_dim,\n",
        "              hid_dim,\n",
        "              dec_layers,\n",
        "              dec_heads,\n",
        "              dec_pf_dim,\n",
        "              enc_dropout,\n",
        "              device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-OBWkeJ2oMO",
        "outputId": "c5f3df27-e9e9-42bd-d713-9efa03b10c30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2SeqTransformer(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(3131, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0-2): 3 x EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(3351, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0-2): 3 x DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=256, out_features=3351, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
        "model.apply(initialize_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3C5swz_2oMP",
        "outputId": "6354a518-01fc-4678-e57b-8f11d2e60336"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "801536\n",
            " 25600\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            "131072\n",
            "   512\n",
            "131072\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            "131072\n",
            "   512\n",
            "131072\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            "131072\n",
            "   512\n",
            "131072\n",
            "   256\n",
            "857856\n",
            " 25600\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            "131072\n",
            "   512\n",
            "131072\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            "131072\n",
            "   512\n",
            "131072\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            "131072\n",
            "   512\n",
            "131072\n",
            "   256\n",
            "857856\n",
            "  3351\n",
            "______\n",
            "6525463\n"
          ]
        }
      ],
      "source": [
        "#we can print the complexity by the number of parameters\n",
        "def count_parameters(model):\n",
        "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
        "    for item in params:\n",
        "        print(f'{item:>6}')\n",
        "    print(f'______\\n{sum(params):>6}')\n",
        "\n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "wgm921uN2oMP"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "lr = 0.0005\n",
        "\n",
        "#training hyperparameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DjmhOOs02oMP"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, src_len, trg in loader:\n",
        "\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "\n",
        "        print(\"Shape of trg before slicing:\", trg.shape)  # Add this line\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "\n",
        "        print(\"Shape of trg after slicing:\", trg[:,:-1].shape)  # Add this line\n",
        "\n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg    = [batch size, trg len]\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output.reshape(-1, output_dim)\n",
        "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
        "\n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg    = [batch size * trg len - 1]\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / loader_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ykmb7ykt2oMP"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader, criterion, loader_length):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for src, src_len, trg in loader:\n",
        "\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "\n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / loader_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "19cdsIB22oMP"
      },
      "outputs": [],
      "source": [
        "train_loader_length = len(list(iter(train_loader)))\n",
        "val_loader_length   = len(list(iter(valid_loader)))\n",
        "test_loader_length  = len(list(iter(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "LPth0Gko2oMQ"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTbI0_DZ2oMQ",
        "outputId": "ecb39e1d-be63-4b83-d5b3-4f14d527c84e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 13])\n",
            "Shape of trg after slicing: torch.Size([64, 12])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 16])\n",
            "Shape of trg after slicing: torch.Size([64, 15])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 13])\n",
            "Shape of trg after slicing: torch.Size([64, 12])\n",
            "Shape of trg before slicing: torch.Size([64, 16])\n",
            "Shape of trg after slicing: torch.Size([64, 15])\n",
            "Shape of trg before slicing: torch.Size([64, 13])\n",
            "Shape of trg after slicing: torch.Size([64, 12])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 16])\n",
            "Shape of trg after slicing: torch.Size([64, 15])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 16])\n",
            "Shape of trg after slicing: torch.Size([64, 15])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 16])\n",
            "Shape of trg after slicing: torch.Size([64, 15])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 16])\n",
            "Shape of trg after slicing: torch.Size([64, 15])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 13])\n",
            "Shape of trg after slicing: torch.Size([64, 12])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 16])\n",
            "Shape of trg after slicing: torch.Size([64, 15])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 13])\n",
            "Shape of trg after slicing: torch.Size([64, 12])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 13])\n",
            "Shape of trg after slicing: torch.Size([64, 12])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 16])\n",
            "Shape of trg after slicing: torch.Size([64, 15])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 16])\n",
            "Shape of trg after slicing: torch.Size([64, 15])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 15])\n",
            "Shape of trg after slicing: torch.Size([64, 14])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 16])\n",
            "Shape of trg after slicing: torch.Size([64, 15])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([64, 14])\n",
            "Shape of trg after slicing: torch.Size([64, 13])\n",
            "Shape of trg before slicing: torch.Size([22, 13])\n",
            "Shape of trg after slicing: torch.Size([22, 12])\n",
            "Epoch: 01 | Time: 0m 10s\n",
            "\tTrain Loss: 5.573 | Train PPL: 263.289\n",
            "\t Val. Loss: 4.666 |  Val. PPL: 106.272\n"
          ]
        }
      ],
      "source": [
        "best_valid_loss = float('inf')\n",
        "num_epochs = 1\n",
        "clip       = 1\n",
        "\n",
        "save_path = f'{model.__class__.__name__}.pt'\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
        "    valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
        "\n",
        "    #for plotting\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "    #lower perplexity is better"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4utlsU-4fKb",
        "outputId": "8a381d42-91c8-40a3-90f9-64579a8b188f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Test Loss: 4.720 | Test PPL: 112.133 |\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(save_path))\n",
        "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KnvH55KX4jF4",
        "outputId": "fa82c34e-c1a3-4b2d-ca2d-41b48416f9e1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'through his management , commissioner monti achiev'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kjVDiTvI4lZq",
        "outputId": "f2ef6728-fe9a-4022-8b05-9cee2c19b8d0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'kommissionär monti har uppnått goda resultat i sit'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtGAXwsJ4oPj",
        "outputId": "45eb1876-4161-4170-af0f-6764299438c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([   2,  459,  257, 1245,    5,   67,  850,    0,    3], device='cuda:0')"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src_text = text_transform[SRC_LANGUAGE](sample[0]).to(device)\n",
        "src_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLV1f9rc4p6T",
        "outputId": "3363ed17-68ed-4e75-a343-de02657eccd6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([   2,   72,  878,   15,  913,  752,  626,   10, 1010,    3],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trg_text = text_transform[TRG_LANGUAGE](sample[1]).to(device)\n",
        "trg_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "9JvH4TOV4rsr"
      },
      "outputs": [],
      "source": [
        "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ul_GW1hx4tYB"
      },
      "outputs": [],
      "source": [
        "trg_text = trg_text.reshape(1, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i-L3hxl4u9_",
        "outputId": "2fd3e017-a027-44c7-e08f-6b3275d69164"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 9]), torch.Size([1, 10]))"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src_text.shape, trg_text.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "fGPas_HG4wAq"
      },
      "outputs": [],
      "source": [
        "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "orfKlr8k4yC3"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(save_path))\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output, attentions = model(src_text, trg_text) #turn off teacher forcing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGud9zFZ4y-Y",
        "outputId": "8fbaa374-d431-45b6-a14f-c306f4dbe56e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 10, 3351])"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.shape #batch_size, trg_len, trg_output_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "JhJ6y90M41EG"
      },
      "outputs": [],
      "source": [
        "output = output.squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5tdDASn42mE",
        "outputId": "a852fbf9-8d44-43f2-fccd-022ba3cbec64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 3351])"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRcd74iq45Lr",
        "outputId": "17840e42-af0c-463c-e573-7ebdebfd96d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([9, 3351])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = output[1:]\n",
        "output.shape #trg_len, trg_output_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "F4HwFCfz47iX"
      },
      "outputs": [],
      "source": [
        "output_max = output.argmax(1) #returns max indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSLrp7Xo48TF",
        "outputId": "b74c9641-c642-4509-ae0e-7cba64550405"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([7, 7, 0, 7, 3, 7, 0, 3, 3], device='cuda:0')"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "okj-Rq5v499K"
      },
      "outputs": [],
      "source": [
        "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxQPHPG4_h8",
        "outputId": "8bec9139-f7c9-4a4a-9df0-91a99e9cabd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ",\n",
            ",\n",
            "<unk>\n",
            ",\n",
            "<eos>\n",
            ",\n",
            "<unk>\n",
            "<eos>\n",
            "<eos>\n"
          ]
        }
      ],
      "source": [
        "for token in output_max:\n",
        "    print(mapping[token.item()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE0wzL1S5CUx",
        "outputId": "b87ef6f2-38fd-41bf-8d30-19367eb54537"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 10, 9])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attentions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU7I_y5C5EQU",
        "outputId": "b546c630-4a6f-437f-a121-90f74a1857cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 9])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attention = attentions[0, 0, :, :]\n",
        "attention.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6bEdGBq5GUQ",
        "outputId": "a7e4cc39-9875-47ba-c239-d7b79b08eb10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<sos>',\n",
              " 'through',\n",
              " 'his',\n",
              " 'management',\n",
              " ',',\n",
              " 'commissioner',\n",
              " 'monti',\n",
              " 'achiev',\n",
              " '<eos>']"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](sample[0]) + ['<eos>']\n",
        "src_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMui72cp5JR-",
        "outputId": "f6566e34-a202-490f-b218-1fe3eba2eb08"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<sos>', ',', ',', '<unk>', ',', '<eos>', ',', '<unk>', '<eos>', '<eos>']"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
        "trg_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "erpEYNmt5KzC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_attention(sentence, translation, attention):\n",
        "\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
        "\n",
        "    cax = ax.matshow(attention, cmap='bone')\n",
        "\n",
        "    ax.tick_params(labelsize=10)\n",
        "\n",
        "    y_ticks =  [''] + translation\n",
        "    x_ticks =  [''] + sentence\n",
        "\n",
        "    ax.set_xticklabels(x_ticks, rotation=45)\n",
        "    ax.set_yticklabels(y_ticks)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "iHEtxqPi5NAN",
        "outputId": "85729c65-2097-467c-f2b0-4e94ffb6fcdf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-58-f6a02a8c45a2>:18: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels(x_ticks, rotation=45)\n",
            "<ipython-input-58-f6a02a8c45a2>:19: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_yticklabels(y_ticks)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display_attention(src_tokens, trg_tokens, attention)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
